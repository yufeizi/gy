#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ä¼ è¾“ç®¡ç†å™¨
åŠŸèƒ½ï¼š
1. å·²æœç´¢å…³é”®è¯å’Œå·²ç‚¹å‡»å•†å“çš„EXCELæ–‡æ¡£ä¼ è¾“åˆ°ä¸»ç›®å½•
2. æ¯éš”10åˆ†é’Ÿå‘ä¸»ç›®å½•æ’é˜Ÿä¼ è¾“
3. EXCELæ–‡æ¡£å¢é‡å¼ä¿å­˜
4. å·²ç‚¹å‡»å•†å“ä¼ è¾“åä¸»ç¨‹åºå›ä¼ è¦†ç›–æœºåˆ¶
"""

import os
import json
import time
import threading
# ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨openpyxlæ›¿ä»£pandasï¼Œå‡å°‘ä¾èµ–
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    try:
        import openpyxl
        HAS_OPENPYXL = True
    except ImportError:
        HAS_OPENPYXL = False
from datetime import datetime
from typing import Dict, List, Set, Optional
from pathlib import Path
import shutil
import platform

# ğŸ”¥ è·¨å¹³å°æ–‡ä»¶é”å¯¼å…¥
try:
    import fcntl  # æ–‡ä»¶é”ï¼ˆLinux/Macï¼‰
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False

try:
    import msvcrt  # æ–‡ä»¶é”ï¼ˆWindowsï¼‰
    HAS_MSVCRT = True
except ImportError:
    HAS_MSVCRT = False


class DataTransferManager:
    """æ•°æ®ä¼ è¾“ç®¡ç†å™¨"""

    def __init__(self, main_dir: str = ".", transfer_interval: int = 600, ui_callback=None):
        """
        åˆå§‹åŒ–æ•°æ®ä¼ è¾“ç®¡ç†å™¨

        Args:
            main_dir: ä¸»ç›®å½•è·¯å¾„
            transfer_interval: ä¼ è¾“é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤600ç§’=10åˆ†é’Ÿ
            ui_callback: UIå›è°ƒå‡½æ•°ï¼Œç”¨äºåˆ·æ–°ç•Œé¢æ˜¾ç¤º
        """
        self.main_dir = Path(main_dir)
        self.transfer_interval = transfer_interval
        self.scripts_dir = self.main_dir / "generated_scripts"
        
        # ä¸»ç›®å½•æ–‡ä»¶
        self.main_excel_file = self.main_dir / "ä¸»æ•°æ®è¡¨.xlsx"
        self.main_clicked_file = self.main_dir / "pdd_automation" / "data" / "main_image_hashes.json"
        self.main_keywords_file = self.main_dir / "å·²æœç´¢å…³é”®è¯.json"
        self.transfer_queue_dir = self.main_dir / "transfer_queue"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.main_dir.mkdir(exist_ok=True)
        self.transfer_queue_dir.mkdir(exist_ok=True)

        # ğŸ”¥ ä¿®å¤ï¼šåˆå§‹åŒ–ä¸»æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self._init_main_files()

        # ä¼ è¾“çŠ¶æ€
        self.is_running = False
        self.transfer_thread = None

        # ğŸ”¥ UIå›è°ƒå‡½æ•°
        self.ui_callback = ui_callback

        print(f"æ•°æ®ä¼ è¾“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ä¸»ç›®å½•: {self.main_dir}")
        print(f"   ä¼ è¾“é—´éš”: {self.transfer_interval}ç§’")
        print(f"   UIå›è°ƒ: {'å·²è®¾ç½®' if ui_callback else 'æœªè®¾ç½®'}")

        # ğŸ”¥ å¼ºåˆ¶ç¡®ä¿å…³é”®æ–¹æ³•å­˜åœ¨
        self._ensure_critical_methods()

    def _ensure_critical_methods(self):
        """ğŸ”¥ ç¡®ä¿å…³é”®æ–¹æ³•å­˜åœ¨ï¼Œé˜²æ­¢åŠ¨æ€åˆ é™¤æˆ–å¯¼å…¥é—®é¢˜"""
        try:
            # æ£€æŸ¥å¹¶ä¿®å¤_refresh_ui_searched_keywordsæ–¹æ³•
            if not hasattr(self, '_refresh_ui_searched_keywords') or not callable(getattr(self, '_refresh_ui_searched_keywords', None)):
                print("âš ï¸ æ£€æµ‹åˆ°_refresh_ui_searched_keywordsæ–¹æ³•ç¼ºå¤±ï¼Œå¼ºåˆ¶ä¿®å¤...")

                def _refresh_ui_searched_keywords_fixed(self):
                    """ğŸ”¥ ä¿®å¤ç‰ˆæœ¬çš„åˆ·æ–°UIæ–¹æ³•"""
                    try:
                        if not self.ui_callback:
                            print("â„¹ï¸ æœªè®¾ç½®UIå›è°ƒï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                            return

                        # è¯»å–ä¸»å·²æœç´¢å…³é”®è¯æ–‡ä»¶
                        if not self.main_keywords_file.exists():
                            print("â„¹ï¸ ä¸»å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                            return

                        import json
                        with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                            keywords_data = json.load(f)

                        searched_keywords = set(keywords_data.get('searched_keywords', []))

                        if not searched_keywords:
                            print("â„¹ï¸ æ²¡æœ‰å·²æœç´¢å…³é”®è¯ï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                            return

                        # ğŸ”¥ è°ƒç”¨UIå›è°ƒå‡½æ•°åˆ·æ–°ç•Œé¢
                        print(f"åˆ·æ–°UIæ˜¾ç¤º: {len(searched_keywords)} ä¸ªå·²æœç´¢å…³é”®è¯")
                        self.ui_callback(searched_keywords)

                    except Exception as e:
                        print(f"âŒ åˆ·æ–°UIæ˜¾ç¤ºå¤±è´¥: {e}")

                # å¼ºåˆ¶ç»‘å®šæ–¹æ³•
                import types
                self._refresh_ui_searched_keywords = types.MethodType(_refresh_ui_searched_keywords_fixed, self)
                print("âœ… å·²å¼ºåˆ¶ä¿®å¤_refresh_ui_searched_keywordsæ–¹æ³•")

            print(f"æ–¹æ³•æ£€æŸ¥å®Œæˆï¼Œ_refresh_ui_searched_keywordså­˜åœ¨: {hasattr(self, '_refresh_ui_searched_keywords')}")

        except Exception as e:
            print(f"âŒ å…³é”®æ–¹æ³•æ£€æŸ¥å¤±è´¥: {e}")

    def _init_main_files(self):
        """ğŸ”¥ åˆå§‹åŒ–ä¸»æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # åˆå§‹åŒ–å·²ç‚¹å‡»å•†å“æ–‡ä»¶
            if not self.main_clicked_file.exists():
                # ç¡®ä¿dataç›®å½•å­˜åœ¨
                self.main_clicked_file.parent.mkdir(exist_ok=True)
                
                initial_clicked_data = {
                    'last_updated': datetime.now().isoformat(),
                    'hashes': [],
                    'total_count': 0,
                    'created': datetime.now().isoformat()
                }
                with open(self.main_clicked_file, 'w', encoding='utf-8') as f:
                    json.dump(initial_clicked_data, f, ensure_ascii=False, indent=2)
                print(f"âœ… åˆå§‹åŒ–ä¸»å·²ç‚¹å‡»å•†å“æ–‡ä»¶: {self.main_clicked_file}")

            # åˆå§‹åŒ–å·²æœç´¢å…³é”®è¯æ–‡ä»¶
            if not self.main_keywords_file.exists():
                initial_keywords_data = {
                    'last_updated': datetime.now().isoformat(),
                    'searched_keywords': [],
                    'total_count': 0,
                    'created': datetime.now().isoformat()
                }
                with open(self.main_keywords_file, 'w', encoding='utf-8') as f:
                    json.dump(initial_keywords_data, f, ensure_ascii=False, indent=2)
                print(f"âœ… åˆå§‹åŒ–ä¸»å·²æœç´¢å…³é”®è¯æ–‡ä»¶: {self.main_keywords_file}")

        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–ä¸»æ–‡ä»¶å¤±è´¥: {e}")

    def start_auto_transfer(self):
        """å¯åŠ¨è‡ªåŠ¨ä¼ è¾“ - æ”¹ä¸ºå¯åŠ¨æ—¶ä¼ è¾“ä¸€æ¬¡å°±åœæ­¢"""
        if self.is_running:
            print("âš ï¸ è‡ªåŠ¨ä¼ è¾“å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        self.transfer_thread = threading.Thread(target=self._transfer_once, daemon=True)
        self.transfer_thread.start()
        print(f"å¯åŠ¨æ—¶ä¼ è¾“å·²å¯åŠ¨ï¼Œå°†æ‰§è¡Œä¸€æ¬¡æ•°æ®ä¼ è¾“åè‡ªåŠ¨åœæ­¢")
    
    def stop_auto_transfer(self):
        """åœæ­¢è‡ªåŠ¨ä¼ è¾“"""
        self.is_running = False
        if self.transfer_thread:
            self.transfer_thread.join(timeout=5)
        print("è‡ªåŠ¨ä¼ è¾“å·²åœæ­¢")
    
    def _transfer_once(self):
        """ğŸ”¥ ä¼˜åŒ–ï¼šå¯åŠ¨æ—¶ä¼ è¾“ä¸€æ¬¡å°±åœæ­¢ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨"""
        try:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] å¼€å§‹å¯åŠ¨æ—¶æ•°æ®ä¼ è¾“...")
            
            # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®
            import gc
            
            # 1. æ”¶é›†æ‰€æœ‰æµè§ˆå™¨æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            transfer_data = self._collect_browser_data_optimized()
            
            # 2. å¤„ç†ä¼ è¾“æ•°æ®
            if transfer_data and transfer_data.get('browsers'):
                print(f"å‘ç° {len(transfer_data['browsers'])} ä¸ªæµè§ˆå™¨çš„æ•°æ®")
                
                # ğŸ”¥ é€ä¸ªå¤„ç†æµè§ˆå™¨æ•°æ®ï¼Œå¤„ç†å®Œä¸€ä¸ªç«‹å³é‡Šæ”¾å†…å­˜
                for browser_id, browser_data in transfer_data['browsers'].items():
                    try:
                        self._process_single_transfer({
                            'browser_id': browser_id,
                            'new_clicked': browser_data.get('new_clicked', []),
                            'new_keywords': browser_data.get('new_keywords', []),
                            'new_products': browser_data.get('new_products', [])
                        })
                        
                        # ğŸ”¥ å¤„ç†å®Œä¸€ä¸ªæµè§ˆå™¨åç«‹å³æ¸…ç†å†…å­˜
                        browser_data.clear()
                        gc.collect()
                        
                    except Exception as browser_e:
                        print(f"âŒ å¤„ç†æµè§ˆå™¨ {browser_id} æ•°æ®å¤±è´¥: {browser_e}")
                        continue
                
                # ğŸ”¥ æ¸…ç†ä¼ è¾“æ•°æ®
                transfer_data.clear()
                gc.collect()
                
                print(f"âœ… å¯åŠ¨æ—¶æ•°æ®ä¼ è¾“å®Œæˆ")
            else:
                print("â„¹ï¸ æ²¡æœ‰å‘ç°éœ€è¦ä¼ è¾“çš„æ•°æ®")
            
        except Exception as e:
            print(f"âŒ å¯åŠ¨æ—¶æ•°æ®ä¼ è¾“å¼‚å¸¸: {e}")
        finally:
            # ä¼ è¾“å®Œæˆåè‡ªåŠ¨åœæ­¢
            self.is_running = False
            print("å¯åŠ¨æ—¶ä¼ è¾“å·²å®Œæˆï¼Œè‡ªåŠ¨åœæ­¢")
            
            # ğŸ”¥ æœ€ç»ˆå†…å­˜æ¸…ç†
            import gc
            collected = gc.collect()
            print(f"ä¼ è¾“å®Œæˆåæ¸…ç†äº† {collected} ä¸ªå¯¹è±¡")
    
    def manual_transfer(self):
        """æ‰‹åŠ¨è§¦å‘ä¼ è¾“ï¼ˆæŒ‰éœ€ä¼ è¾“ï¼‰"""
        if self.is_running:
            print("âš ï¸ ä¼ è¾“æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ")
            return False
        
        print("æ‰‹åŠ¨è§¦å‘æ•°æ®ä¼ è¾“...")
        self.start_auto_transfer()
        return True
    
    def _process_pending_transfers(self):
        """å¤„ç†å¾…å¤„ç†çš„ä¼ è¾“ä»»åŠ¡"""
        try:
            if not self.transfer_queue_dir.exists():
                return
                
            # æŸ¥æ‰¾ä¼ è¾“é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
            transfer_files = list(self.transfer_queue_dir.glob("transfer_*.json"))
            
            for transfer_file in transfer_files:
                try:
                    print(f"å¤„ç†ä¼ è¾“ä»»åŠ¡: {transfer_file.name}")
                    
                    # è¯»å–ä¼ è¾“æ•°æ®
                    with open(transfer_file, 'r', encoding='utf-8') as f:
                        transfer_data = json.load(f)
                    
                    # å¤„ç†ä¼ è¾“ä»»åŠ¡
                    self._process_single_transfer(transfer_data)
                    
                    # åˆ é™¤å·²å¤„ç†çš„ä¼ è¾“æ–‡ä»¶
                    transfer_file.unlink()
                    print(f"âœ… ä¼ è¾“ä»»åŠ¡å®Œæˆ: {transfer_file.name}")
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†ä¼ è¾“ä»»åŠ¡å¤±è´¥ {transfer_file.name}: {e}")
                    
        except Exception as e:
            print(f"âŒ å¤„ç†å¾…å¤„ç†ä¼ è¾“å¤±è´¥: {e}")
    
    def _process_single_transfer(self, transfer_data: Dict):
        """å¤„ç†å•ä¸ªä¼ è¾“ä»»åŠ¡"""
        try:
            browser_id = transfer_data.get('browser_id', 'unknown')
            print(f"å¤„ç†æµè§ˆå™¨ {browser_id} çš„ä¼ è¾“æ•°æ®")
            
            # 1. å¤„ç†å·²ç‚¹å‡»å•†å“ï¼ˆå¢é‡å¼ä¿å­˜ï¼‰
            if 'new_clicked' in transfer_data and transfer_data['new_clicked']:
                self._incremental_save_clicked_products(browser_id, transfer_data['new_clicked'])
            
            # 2. å¤„ç†å·²æœç´¢å…³é”®è¯ï¼ˆå¢é‡å¼ä¿å­˜ï¼‰
            if 'new_keywords' in transfer_data and transfer_data['new_keywords']:
                self._incremental_save_searched_keywords(browser_id, transfer_data['new_keywords'])
            
            # 3. å¤„ç†æ–°å•†å“æ•°æ®ï¼ˆå¢é‡å¼ä¿å­˜ï¼‰
            if 'new_products' in transfer_data and transfer_data['new_products']:
                self._incremental_save_new_products(browser_id, transfer_data['new_products'])
            
            print(f"âœ… æµè§ˆå™¨ {browser_id} ä¼ è¾“æ•°æ®å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ å¤„ç†ä¼ è¾“æ•°æ®å¤±è´¥: {e}")
    
    def _incremental_save_clicked_products(self, browser_id: str, new_clicked: List[str]):
        """å¢é‡å¼ä¿å­˜å·²ç‚¹å‡»å•†å“"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = {}
            if self.main_clicked_file.exists():
                with open(self.main_clicked_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            
            # è·å–ç°æœ‰å“ˆå¸Œå€¼
            existing_hashes = set(existing_data.get('hashes', []))
            
            # æ·»åŠ æ–°çš„å“ˆå¸Œå€¼
            new_hashes = set(new_clicked)
            combined_hashes = existing_hashes | new_hashes
            
            # è®¡ç®—æ–°å¢æ•°é‡
            added_count = len(combined_hashes - existing_hashes)
            
            # ä¿å­˜å¢é‡æ•°æ®
            updated_data = {
                'last_updated': datetime.now().isoformat(),
                'hashes': sorted(list(combined_hashes)),
                'total_count': len(combined_hashes),
                'new_added': added_count,
                'last_browser_update': browser_id
            }
            
            with open(self.main_clicked_file, 'w', encoding='utf-8') as f:
                json.dump(updated_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å·²ç‚¹å‡»å•†å“å¢é‡ä¿å­˜å®Œæˆ: æ–°å¢ {added_count} ä¸ªï¼Œæ€»è®¡ {len(combined_hashes)} ä¸ª")
            
        except Exception as e:
            print(f"âŒ å¢é‡ä¿å­˜å·²ç‚¹å‡»å•†å“å¤±è´¥: {e}")
    
    def _incremental_save_searched_keywords(self, browser_id: str, new_keywords: List[str]):
        """å¢é‡å¼ä¿å­˜å·²æœç´¢å…³é”®è¯"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_data = {}
            if self.main_keywords_file.exists():
                with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            
            # è·å–ç°æœ‰å…³é”®è¯
            existing_keywords = set(existing_data.get('searched_keywords', []))
            
            # æ·»åŠ æ–°çš„å…³é”®è¯
            new_keywords_set = set(new_keywords)
            combined_keywords = existing_keywords | new_keywords_set
            
            # è®¡ç®—æ–°å¢æ•°é‡
            added_count = len(combined_keywords - existing_keywords)
            
            # ä¿å­˜å¢é‡æ•°æ®
            updated_data = {
                'last_updated': datetime.now().isoformat(),
                'searched_keywords': sorted(list(combined_keywords)),
                'total_count': len(combined_keywords),
                'new_added': added_count,
                'last_browser_update': browser_id
            }
            
            with open(self.main_keywords_file, 'w', encoding='utf-8') as f:
                json.dump(updated_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å·²æœç´¢å…³é”®è¯å¢é‡ä¿å­˜å®Œæˆ: æ–°å¢ {added_count} ä¸ªï¼Œæ€»è®¡ {len(combined_keywords)} ä¸ª")
            
            # ğŸ”¥ ä¼ è¾“å®Œæˆåï¼Œæ¸…ç†æµè§ˆå™¨ç«¯çš„å·²æœç´¢å…³é”®è¯
            self._clean_browser_searched_keywords(browser_id, new_keywords)
            
            # åˆ·æ–°UIæ˜¾ç¤º
            if self.ui_callback:
                self.ui_callback(combined_keywords)
            
        except Exception as e:
            print(f"âŒ å¢é‡ä¿å­˜å·²æœç´¢å…³é”®è¯å¤±è´¥: {e}")
    
    def _clean_browser_searched_keywords(self, browser_id: str, searched_keywords: List[str]):
        """ğŸ”¥ æ¸…ç†æµè§ˆå™¨ç«¯çš„å·²æœç´¢å…³é”®è¯ï¼ˆä¼ è¾“ååˆ é™¤ï¼‰"""
        try:
            # æ‰¾åˆ°æµè§ˆå™¨ç›®å½•
            browser_dir = self.scripts_dir / f"browser_{browser_id}"
            if not browser_dir.exists():
                print(f"âš ï¸ æµè§ˆå™¨ç›®å½•ä¸å­˜åœ¨: {browser_dir}")
                return
            
            # è¯»å–æµè§ˆå™¨çš„é…ç½®æ–‡ä»¶
            config_file = browser_dir / "config_api.json"
            if not config_file.exists():
                print(f"âš ï¸ æµè§ˆå™¨é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                return
            
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # è·å–å½“å‰å…³é”®è¯åˆ—è¡¨
            current_keywords = config.get('parse_settings', {}).get('search_keywords', [])
            if not current_keywords:
                print(f"â„¹ï¸ æµè§ˆå™¨ {browser_id} æ²¡æœ‰æœç´¢å…³é”®è¯")
                return
            
            # æ¸…ç†å·²æœç´¢çš„å…³é”®è¯
            cleaned_keywords = []
            removed_count = 0
            
            for keyword in current_keywords:
                # å¦‚æœå…³é”®è¯ä»¥"---å·²æœç´¢"ç»“å°¾ï¼Œåˆ™åˆ é™¤
                if keyword.endswith('---å·²æœç´¢'):
                    removed_count += 1
                    print(f"åˆ é™¤å·²æœç´¢å…³é”®è¯: {keyword}")
                else:
                    cleaned_keywords.append(keyword)
            
            # æ›´æ–°é…ç½®æ–‡ä»¶
            if 'parse_settings' not in config:
                config['parse_settings'] = {}
            
            config['parse_settings']['search_keywords'] = cleaned_keywords
            
            # ä¿å­˜æ›´æ–°åçš„é…ç½®
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æµè§ˆå™¨ {browser_id} å·²æœç´¢å…³é”®è¯æ¸…ç†å®Œæˆ: åˆ é™¤äº† {removed_count} ä¸ªå…³é”®è¯")
            
        except Exception as e:
            print(f"âŒ æ¸…ç†æµè§ˆå™¨å·²æœç´¢å…³é”®è¯å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    def _incremental_save_new_products(self, browser_id: str, new_products: List[Dict]):
        """å¢é‡å¼ä¿å­˜æ–°å•†å“æ•°æ®"""
        try:
            # è¿™é‡Œå¯ä»¥æ·»åŠ Excelå¢é‡ä¿å­˜é€»è¾‘
            # æš‚æ—¶åªè®°å½•æ—¥å¿—
            print(f"æ–°å•†å“æ•°æ®: {len(new_products)} ä¸ªï¼ˆæ¥è‡ªæµè§ˆå™¨ {browser_id}ï¼‰")
            
        except Exception as e:
            print(f"âŒ å¢é‡ä¿å­˜æ–°å•†å“æ•°æ®å¤±è´¥: {e}")
    
    def receive_browser_data(self, browser_id: str, data: Dict) -> bool:
        """ğŸ”¥ æ¥æ”¶æµè§ˆå™¨ä¸»åŠ¨ä¼ è¾“çš„æ•°æ®"""
        try:
            print(f"æ¥æ”¶æµè§ˆå™¨ {browser_id} çš„ä¼ è¾“æ•°æ®")
            
            # åˆ›å»ºä¼ è¾“ä»»åŠ¡æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_filename = f"transfer_{timestamp}_{browser_id}.json"
            task_file = self.transfer_queue_dir / task_filename
            
            # æ·»åŠ æµè§ˆå™¨IDå’Œæ—¶é—´æˆ³
            transfer_data = {
                'browser_id': browser_id,
                'timestamp': datetime.now().isoformat(),
                'new_clicked': data.get('hashes', []),
                'new_keywords': data.get('searched_keywords', []),
                'new_products': data.get('new_products', [])
            }
            
            # ä¿å­˜ä¼ è¾“ä»»åŠ¡
            with open(task_file, 'w', encoding='utf-8') as f:
                json.dump(transfer_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ä¼ è¾“ä»»åŠ¡å·²åˆ›å»º: {task_filename}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¥æ”¶æµè§ˆå™¨æ•°æ®å¤±è´¥: {e}")
            return False
    
    def get_transfer_status(self) -> Dict:
        """ğŸ”¥ è·å–ä¼ è¾“çŠ¶æ€ä¿¡æ¯"""
        try:
            status = {
                'is_running': self.is_running,
                'transfer_interval': self.transfer_interval,
                'last_check': getattr(self, '_last_check_time', 'æœªå¼€å§‹'),
                'queue_count': 0,
                'main_files': {}
            }
            
            # ç»Ÿè®¡ä¼ è¾“é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ•°é‡
            if self.transfer_queue_dir.exists():
                transfer_files = list(self.transfer_queue_dir.glob("transfer_*.json"))
                status['queue_count'] = len(transfer_files)
            
            # æ£€æŸ¥ä¸»æ–‡ä»¶çŠ¶æ€
            if self.main_clicked_file.exists():
                with open(self.main_clicked_file, 'r', encoding='utf-8') as f:
                    clicked_data = json.load(f)
                    status['main_files']['clicked_products'] = {
                        'total_count': clicked_data.get('total_count', 0),
                        'last_updated': clicked_data.get('last_updated', 'æœªçŸ¥')
                    }
            
            if self.main_keywords_file.exists():
                with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                    keywords_data = json.load(f)
                    status['main_files']['searched_keywords'] = {
                        'total_count': keywords_data.get('total_count', 0),
                        'last_updated': keywords_data.get('last_updated', 'æœªçŸ¥')
                    }
            
            return status
            
        except Exception as e:
            print(f"âŒ è·å–ä¼ è¾“çŠ¶æ€å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _collect_browser_data(self) -> Dict:
        """æ”¶é›†æ‰€æœ‰æµè§ˆå™¨çš„æ•°æ®"""
        transfer_data = {
            'timestamp': datetime.now().isoformat(),
            'browsers': {},
            'total_new_products': 0,
            'total_new_clicked': 0,
            'total_new_keywords': 0
        }
        
        if not self.scripts_dir.exists():
            return transfer_data
        
        for browser_dir in self.scripts_dir.iterdir():
            if browser_dir.is_dir() and browser_dir.name.startswith('browser_'):
                browser_id = browser_dir.name.replace('browser_', '')
                browser_data = self._collect_single_browser_data(browser_dir, browser_id)
                
                if browser_data and isinstance(browser_data, dict):
                    transfer_data['browsers'][browser_id] = browser_data
                    transfer_data['total_new_products'] += len(browser_data.get('new_products', []))
                    transfer_data['total_new_clicked'] += len(browser_data.get('new_clicked', []))
                    transfer_data['total_new_keywords'] += len(browser_data.get('new_keywords', []))
                elif browser_data is not None:
                    print(f"âš ï¸ æµè§ˆå™¨ {browser_id} æ•°æ®æ ¼å¼é”™è¯¯: {type(browser_data)}")
        
        return transfer_data
    
    def _collect_browser_data_optimized(self) -> Dict:
        """ğŸ”¥ ä¼˜åŒ–ç‰ˆï¼šå†…å­˜å‹å¥½çš„æµè§ˆå™¨æ•°æ®æ”¶é›†"""
        transfer_data = {
            'timestamp': datetime.now().isoformat(),
            'browsers': {},
            'total_new_products': 0,
            'total_new_clicked': 0,
            'total_new_keywords': 0
        }
        
        if not self.scripts_dir.exists():
            return transfer_data
        
        import gc
        
        for browser_dir in self.scripts_dir.iterdir():
            if browser_dir.is_dir() and browser_dir.name.startswith('browser_'):
                browser_id = browser_dir.name.replace('browser_', '')
                
                # ğŸ”¥ å•ç‹¬å¤„ç†æ¯ä¸ªæµè§ˆå™¨ï¼Œé¿å…å†…å­˜ç§¯ç´¯
                browser_data = self._collect_single_browser_data_optimized(browser_dir, browser_id)
                
                if browser_data and isinstance(browser_data, dict):
                    if (browser_data.get('new_products') or 
                        browser_data.get('new_clicked') or 
                        browser_data.get('new_keywords')):
                        
                        transfer_data['browsers'][browser_id] = browser_data
                        transfer_data['total_new_products'] += len(browser_data.get('new_products', []))
                        transfer_data['total_new_clicked'] += len(browser_data.get('new_clicked', []))
                        transfer_data['total_new_keywords'] += len(browser_data.get('new_keywords', []))
                    
                    # ğŸ”¥ ç«‹å³æ¸…ç†ä¸éœ€è¦çš„ä¸­é—´æ•°æ®
                    gc.collect()
                elif browser_data is not None:
                    print(f"âš ï¸ æµè§ˆå™¨ {browser_id} æ•°æ®æ ¼å¼é”™è¯¯: {type(browser_data)}")
        
        return transfer_data
    
    def _collect_single_browser_data_optimized(self, browser_dir: Path, browser_id: str) -> Optional[Dict]:
        """ğŸ”¥ ä¼˜åŒ–ç‰ˆï¼šå†…å­˜å‹å¥½çš„å•ä¸ªæµè§ˆå™¨æ•°æ®æ”¶é›†"""
        try:
            browser_data = {
                'browser_id': browser_id,
                'browser_dir': str(browser_dir),
                'new_products': [],
                'new_clicked': [],
                'new_keywords': []
            }
            
            # 1. ğŸ”¥ è½»é‡çº§Excelæ•°æ®æ”¶é›†ï¼ˆåªæ”¶é›†å¿…è¦ä¿¡æ¯ï¼‰
            excel_files = list(browser_dir.glob("output/cj*.xlsx"))
            if excel_files:
                # åªå¤„ç†æœ€æ–°çš„æ–‡ä»¶ï¼Œé¿å…å†…å­˜è¿‡è½½
                latest_excel = max(excel_files, key=lambda f: f.stat().st_mtime)
                new_products = self._extract_new_products_from_excel_lightweight(latest_excel)
                browser_data['new_products'] = new_products[:100]  # é™åˆ¶æ•°é‡ï¼Œé¿å…å†…å­˜è¿‡è½½
            
            # 2. ğŸ”¥ è½»é‡çº§å·²ç‚¹å‡»å•†å“æ”¶é›†
            clicked_file = browser_dir / "data" / "main_image_hashes.json"
            if clicked_file.exists() and clicked_file.stat().st_size < 10 * 1024 * 1024:  # é™åˆ¶æ–‡ä»¶å¤§å°10MB
                new_clicked = self._extract_new_clicked_products(clicked_file)
                browser_data['new_clicked'] = new_clicked[:1000]  # é™åˆ¶æ•°é‡
            
            # 3. ğŸ”¥ è½»é‡çº§å·²æœç´¢å…³é”®è¯æ”¶é›†
            config_file = browser_dir / "config_api.json"
            if config_file.exists() and config_file.stat().st_size < 1024 * 1024:  # é™åˆ¶æ–‡ä»¶å¤§å°1MB
                new_keywords = self._extract_new_searched_keywords(config_file)
                browser_data['new_keywords'] = new_keywords[:50]  # é™åˆ¶æ•°é‡
            
            # åªè¿”å›æœ‰æ–°æ•°æ®çš„æµè§ˆå™¨
            if (browser_data['new_products'] or 
                browser_data['new_clicked'] or 
                browser_data['new_keywords']):
                return browser_data
            
            return None
            
        except Exception as e:
            print(f"âŒ æ”¶é›†æµè§ˆå™¨ {browser_id} æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_new_products_from_excel_lightweight(self, excel_file: Path) -> List[Dict]:
        """ğŸ”¥ è½»é‡çº§Excelæ•°æ®æå–ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…å¤„ç†è¿‡å¤§çš„æ–‡ä»¶
            file_size = excel_file.stat().st_size
            if file_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
                print(f"âš ï¸ Excelæ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡å¤„ç†: {excel_file}")
                return []
            
            if HAS_PANDAS:
                # ä½¿ç”¨pandasåˆ†å—è¯»å–ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
                try:
                    df = pd.read_excel(excel_file, chunksize=100)  # åˆ†å—è¯»å–
                    existing_ids = self._get_existing_product_ids()
                    new_products = []
                    
                    for chunk in df:
                        for _, row in chunk.iterrows():
                            if len(new_products) >= 50:  # é™åˆ¶å¤„ç†æ•°é‡
                                break
                            product_id = str(row.get('å•†å“ID', ''))
                            if product_id and product_id not in existing_ids:
                                new_products.append(row.to_dict())
                        if len(new_products) >= 50:
                            break
                    
                    return new_products
                except:
                    # å¦‚æœåˆ†å—è¯»å–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•ä½†é™åˆ¶è¡Œæ•°
                    df = pd.read_excel(excel_file, nrows=200)  # åªè¯»å‰200è¡Œ
                    existing_ids = self._get_existing_product_ids()
                    new_products = []
                    for _, row in df.iterrows():
                        product_id = str(row.get('å•†å“ID', ''))
                        if product_id and product_id not in existing_ids:
                            new_products.append(row.to_dict())
                    return new_products
            else:
                # åŸæœ‰é€»è¾‘ä¿æŒä¸å˜
                return self._extract_new_products_from_excel(excel_file)
                
        except Exception as e:
            print(f"âŒ è½»é‡çº§æå–Excelæ–°å•†å“å¤±è´¥: {e}")
            return []
    
    def _collect_single_browser_data(self, browser_dir: Path, browser_id: str) -> Optional[Dict]:
        """æ”¶é›†å•ä¸ªæµè§ˆå™¨çš„æ•°æ®"""
        try:
            browser_data = {
                'browser_id': browser_id,
                'browser_dir': str(browser_dir),
                'new_products': [],
                'new_clicked': [],
                'new_keywords': []
            }
            
            # 1. æ”¶é›†EXCELæ•°æ®ï¼ˆæ–°å¢å•†å“ï¼‰
            excel_files = list(browser_dir.glob("output/cj*.xlsx"))
            for excel_file in excel_files:
                if excel_file.exists():
                    new_products = self._extract_new_products_from_excel(excel_file)
                    browser_data['new_products'].extend(new_products)
            
            # 2. æ”¶é›†å·²ç‚¹å‡»å•†å“
            clicked_file = browser_dir / "data" / "main_image_hashes.json"
            if clicked_file.exists():
                new_clicked = self._extract_new_clicked_products(clicked_file)
                browser_data['new_clicked'] = new_clicked
            
            # 3. æ”¶é›†å·²æœç´¢å…³é”®è¯
            config_file = browser_dir / "config_api.json"
            if config_file.exists():
                new_keywords = self._extract_new_searched_keywords(config_file)
                browser_data['new_keywords'] = new_keywords
            
            # åªè¿”å›æœ‰æ–°æ•°æ®çš„æµè§ˆå™¨
            if (browser_data['new_products'] or 
                browser_data['new_clicked'] or 
                browser_data['new_keywords']):
                return browser_data
            
            return None
            
        except Exception as e:
            print(f"âŒ æ”¶é›†æµè§ˆå™¨ {browser_id} æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_new_products_from_excel(self, excel_file: Path) -> List[Dict]:
        """ä»EXCELæ–‡ä»¶æå–æ–°å•†å“æ•°æ®"""
        try:
            if HAS_PANDAS:
                # ä½¿ç”¨pandasè¯»å–
                df = pd.read_excel(excel_file)
                existing_ids = self._get_existing_product_ids()
                new_products = []
                for _, row in df.iterrows():
                    product_id = str(row.get('å•†å“ID', ''))
                    if product_id and product_id not in existing_ids:
                        new_products.append(row.to_dict())
                return new_products
            elif HAS_OPENPYXL:
                # ä½¿ç”¨openpyxlè¯»å–
                from openpyxl import load_workbook
                wb = load_workbook(excel_file, read_only=True)
                ws = wb.active
                
                # è·å–åˆ—æ ‡é¢˜
                headers = [cell.value for cell in ws[1]]
                product_id_col = None
                for i, header in enumerate(headers):
                    if header == 'å•†å“ID':
                        product_id_col = i + 1
                        break
                
                if product_id_col is None:
                    print("âŒ æœªæ‰¾åˆ°å•†å“IDåˆ—")
                    return []
                
                existing_ids = self._get_existing_product_ids()
                new_products = []
                
                for row in ws.iter_rows(min_row=2):
                    product_id = str(row[product_id_col - 1].value) if row[product_id_col - 1].value else ''
                    if product_id and product_id not in existing_ids:
                        product_data = {}
                        for i, cell in enumerate(row):
                            if i < len(headers):
                                product_data[headers[i]] = cell.value
                        new_products.append(product_data)
                
                wb.close()
                return new_products
            else:
                print("âŒ ç¼ºå°‘pandaså’Œopenpyxlï¼Œæ— æ³•è¯»å–Excelæ–‡ä»¶")
                return []
            
        except Exception as e:
            print(f"âŒ æå–EXCELæ–°å•†å“å¤±è´¥: {e}")
            return []
    
    def _extract_new_clicked_products(self, clicked_file: Path) -> List[str]:
        """æå–æ–°çš„å·²ç‚¹å‡»å•†å“ID"""
        try:
            with open(clicked_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ğŸ”¥ æ£€æŸ¥dataçš„ç±»å‹ï¼Œç¡®ä¿æ˜¯å­—å…¸
            if not isinstance(data, dict):
                print(f"âš ï¸ main_image_hashes.jsonæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›å­—å…¸ä½†å¾—åˆ°: {type(data)}")
                print(f"âš ï¸ æ–‡ä»¶å†…å®¹: {data[:100] if isinstance(data, list) else str(data)[:100]}...")
                return []
            
            browser_clicked = set(data.get('hashes', []))
            main_clicked = self._get_existing_clicked_ids()
            
            # è¿”å›æ–°çš„å·²ç‚¹å‡»å•†å“ID
            new_clicked = list(browser_clicked - main_clicked)
            return new_clicked
            
        except Exception as e:
            print(f"âŒ æå–æ–°ç‚¹å‡»å•†å“å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []
    
    def _extract_new_searched_keywords(self, config_file: Path) -> List[str]:
        """æå–æ–°çš„å·²æœç´¢å…³é”®è¯"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # ğŸ”¥ æ£€æŸ¥configçš„ç±»å‹ï¼Œç¡®ä¿æ˜¯å­—å…¸
            if not isinstance(config, dict):
                print(f"âš ï¸ config_api.jsonæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›å­—å…¸ä½†å¾—åˆ°: {type(config)}")
                return []
            
            keywords = config.get('parse_settings', {}).get('search_keywords', [])
            searched_keywords = [kw.replace('---å·²æœç´¢', '') for kw in keywords if kw.endswith('---å·²æœç´¢')]
            
            # è·å–ä¸»æ–‡ä»¶ä¸­å·²æœ‰çš„å…³é”®è¯
            existing_keywords = self._get_existing_searched_keywords()
            
            # è¿”å›æ–°çš„å·²æœç´¢å…³é”®è¯
            new_keywords = [kw for kw in searched_keywords if kw not in existing_keywords]
            return new_keywords
            
        except Exception as e:
            print(f"âŒ æå–æ–°æœç´¢å…³é”®è¯å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []
    
    def _get_existing_product_ids(self) -> Set[str]:
        """è·å–ä¸»æ•°æ®è¡¨ä¸­å·²æœ‰çš„å•†å“ID"""
        try:
            if not self.main_excel_file.exists():
                return set()
                
            if HAS_PANDAS:
                # ä½¿ç”¨pandasè¯»å–
                df = pd.read_excel(self.main_excel_file)
                return set(str(id) for id in df.get('å•†å“ID', []) if pd.notna(id))
            elif HAS_OPENPYXL:
                # ä½¿ç”¨openpyxlè¯»å–
                from openpyxl import load_workbook
                wb = load_workbook(self.main_excel_file, read_only=True)
                ws = wb.active
                
                # è·å–åˆ—æ ‡é¢˜
                headers = [cell.value for cell in ws[1]]
                product_id_col = None
                for i, header in enumerate(headers):
                    if header == 'å•†å“ID':
                        product_id_col = i + 1
                        break
                
                if product_id_col is None:
                    wb.close()
                    return set()
                
                existing_ids = set()
                for row in ws.iter_rows(min_row=2):
                    product_id = row[product_id_col - 1].value
                    if product_id:
                        existing_ids.add(str(product_id))
                
                wb.close()
                return existing_ids
            else:
                print("âŒ ç¼ºå°‘pandaså’Œopenpyxlï¼Œæ— æ³•è¯»å–Excelæ–‡ä»¶")
                return set()
                
        except Exception as e:
            print(f"âŒ è·å–å·²æœ‰å•†å“IDå¤±è´¥: {e}")
            return set()
    
    def _get_existing_clicked_ids(self) -> Set[str]:
        """è·å–ä¸»æ–‡ä»¶ä¸­å·²æœ‰çš„å·²ç‚¹å‡»å•†å“ID"""
        try:
            if self.main_clicked_file.exists():
                with open(self.main_clicked_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return set(data.get('hashes', []))
            return set()
        except Exception as e:
            print(f"âŒ è·å–å·²æœ‰ç‚¹å‡»IDå¤±è´¥: {e}")
            return set()
    
    def _get_existing_searched_keywords(self) -> Set[str]:
        """è·å–ä¸»æ–‡ä»¶ä¸­å·²æœ‰çš„å·²æœç´¢å…³é”®è¯"""
        try:
            if self.main_keywords_file.exists():
                with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return set(data.get('searched_keywords', []))
            return set()
        except Exception as e:
            print(f"âŒ è·å–å·²æœ‰æœç´¢å…³é”®è¯å¤±è´¥: {e}")
            return set()
    
    def _create_transfer_task(self, transfer_data: Dict) -> str:
        """åˆ›å»ºä¼ è¾“ä»»åŠ¡"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        task_id = f"transfer_{timestamp}"
        
        task_file = self.transfer_queue_dir / f"{task_id}.json"
        with open(task_file, 'w', encoding='utf-8') as f:
            json.dump(transfer_data, f, ensure_ascii=False, indent=2)
        
        return task_id

    def _process_transfer_task(self, task_id: str, transfer_data: Dict):
        """å¤„ç†ä¼ è¾“ä»»åŠ¡"""
        try:
            print(f"å¤„ç†ä¼ è¾“ä»»åŠ¡: {task_id}")

            # 1. æ›´æ–°ä¸»EXCELæ–‡ä»¶ï¼ˆå¢é‡å¼ï¼‰
            if transfer_data['total_new_products'] > 0:
                self._update_main_excel(transfer_data)

            # 2. æ›´æ–°å·²ç‚¹å‡»å•†å“æ–‡ä»¶
            if transfer_data['total_new_clicked'] > 0:
                self._update_main_clicked(transfer_data)

            # 3. æ›´æ–°å·²æœç´¢å…³é”®è¯æ–‡ä»¶
            if transfer_data['total_new_keywords'] > 0:
                self._update_main_keywords(transfer_data)

            # 4. å›ä¼ å·²ç‚¹å‡»å•†å“åˆ°å„æµè§ˆå™¨ï¼ˆè¦†ç›–æœºåˆ¶ï¼‰
            self._distribute_clicked_products()

            # 5. æ¸…ç†ä¼ è¾“ä»»åŠ¡
            task_file = self.transfer_queue_dir / f"{task_id}.json"
            if task_file.exists():
                task_file.unlink()

            # 6. ğŸ”¥ åˆ·æ–°UIæ˜¾ç¤ºå·²æœç´¢å…³é”®è¯
            try:
                # ğŸ”¥ åŠ¨æ€ä¿®å¤ï¼šå¦‚æœæ–¹æ³•ä¸å­˜åœ¨ï¼ŒåŠ¨æ€æ·»åŠ 
                if not hasattr(self, '_refresh_ui_searched_keywords'):
                    print(f"âš ï¸ æ£€æµ‹åˆ°æ–¹æ³•ç¼ºå¤±ï¼ŒåŠ¨æ€ä¿®å¤...")

                    # åŠ¨æ€æ·»åŠ ç¼ºå¤±çš„æ–¹æ³•
                    def _refresh_ui_searched_keywords_dynamic(self):
                        """ğŸ”¥ åŠ¨æ€æ·»åŠ çš„åˆ·æ–°UIæ–¹æ³•"""
                        try:
                            if not self.ui_callback:
                                print("â„¹ï¸ æœªè®¾ç½®UIå›è°ƒï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                                return

                            # è¯»å–ä¸»å·²æœç´¢å…³é”®è¯æ–‡ä»¶
                            if not self.main_keywords_file.exists():
                                print("â„¹ï¸ ä¸»å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                                return

                            import json
                            with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                                keywords_data = json.load(f)

                            searched_keywords = set(keywords_data.get('searched_keywords', []))

                            if not searched_keywords:
                                print("â„¹ï¸ æ²¡æœ‰å·²æœç´¢å…³é”®è¯ï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                                return

                            # ğŸ”¥ è°ƒç”¨UIå›è°ƒå‡½æ•°åˆ·æ–°ç•Œé¢
                            print(f"åˆ·æ–°UIæ˜¾ç¤º: {len(searched_keywords)} ä¸ªå·²æœç´¢å…³é”®è¯")
                            self.ui_callback(searched_keywords)

                        except Exception as e:
                            print(f"âŒ åˆ·æ–°UIæ˜¾ç¤ºå¤±è´¥: {e}")

                    # ç»‘å®šæ–¹æ³•åˆ°å®ä¾‹
                    import types
                    self._refresh_ui_searched_keywords = types.MethodType(_refresh_ui_searched_keywords_dynamic, self)
                    print(f"âœ… å·²åŠ¨æ€ä¿®å¤_refresh_ui_searched_keywordsæ–¹æ³•")

                self._refresh_ui_searched_keywords()
            except Exception as refresh_e:
                print(f"âŒ åˆ·æ–°UIå¤±è´¥: {refresh_e}")
                import traceback
                print(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

            print(f"âœ… ä¼ è¾“ä»»åŠ¡å®Œæˆ: {task_id}")

        except Exception as e:
            print(f"âŒ å¤„ç†ä¼ è¾“ä»»åŠ¡å¤±è´¥: {e}")
            import traceback
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    def _update_main_excel(self, transfer_data: Dict):
        """æ›´æ–°ä¸»EXCELæ–‡ä»¶ï¼ˆå¢é‡å¼ä¿å­˜ï¼‰"""
        try:
            print(f"æ›´æ–°ä¸»EXCELæ–‡ä»¶...")

            # æ”¶é›†æ‰€æœ‰æ–°å•†å“æ•°æ®
            all_new_products = []
            for browser_data in transfer_data['browsers'].values():
                all_new_products.extend(browser_data['new_products'])

            if not all_new_products:
                return

            # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å®‰å…¨
            with self._file_lock(self.main_excel_file):
                if HAS_PANDAS:
                    # ä½¿ç”¨pandaså¤„ç†
                    if self.main_excel_file.exists():
                        existing_df = pd.read_excel(self.main_excel_file)
                    else:
                        existing_df = pd.DataFrame()

                    new_df = pd.DataFrame(all_new_products)
                    
                    if not existing_df.empty:
                        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    else:
                        combined_df = new_df

                    if 'å•†å“ID' in combined_df.columns:
                        combined_df = combined_df.drop_duplicates(subset=['å•†å“ID'], keep='last')

                    combined_df.to_excel(self.main_excel_file, index=False, engine='openpyxl')
                    
                elif HAS_OPENPYXL:
                    # ä½¿ç”¨openpyxlå¤„ç†
                    from openpyxl import Workbook, load_workbook
                    
                    if self.main_excel_file.exists():
                        # è¯»å–ç°æœ‰æ•°æ®
                        wb = load_workbook(self.main_excel_file)
                        ws = wb.active
                        
                        # è·å–ç°æœ‰æ•°æ®
                        existing_data = []
                        headers = [cell.value for cell in ws[1]]
                        
                        for row in ws.iter_rows(min_row=2):
                            row_data = {}
                            for i, cell in enumerate(row):
                                if i < len(headers):
                                    row_data[headers[i]] = cell.value
                            existing_data.append(row_data)
                        
                        wb.close()
                    else:
                        existing_data = []
                        headers = list(all_new_products[0].keys()) if all_new_products else []
                    
                    # åˆå¹¶æ•°æ®
                    combined_data = existing_data + all_new_products
                    
                    # å»é‡ï¼ˆåŸºäºå•†å“IDï¼‰
                    if 'å•†å“ID' in headers:
                        seen_ids = set()
                        unique_data = []
                        for item in combined_data:
                            product_id = str(item.get('å•†å“ID', ''))
                            if product_id and product_id not in seen_ids:
                                seen_ids.add(product_id)
                                unique_data.append(item)
                        combined_data = unique_data
                    
                    # ä¿å­˜æ•°æ®
                    wb = Workbook()
                    ws = wb.active
                    
                    # å†™å…¥æ ‡é¢˜è¡Œ
                    for col, header in enumerate(headers, 1):
                        ws.cell(row=1, column=col, value=header)
                    
                    # å†™å…¥æ•°æ®è¡Œ
                    for row_idx, row_data in enumerate(combined_data, 2):
                        for col_idx, header in enumerate(headers, 1):
                            ws.cell(row=row_idx, column=col_idx, value=row_data.get(header, ''))
                    
                    wb.save(self.main_excel_file)
                    wb.close()
                    
                else:
                    print("âŒ ç¼ºå°‘pandaså’Œopenpyxlï¼Œæ— æ³•ä¿å­˜Excelæ–‡ä»¶")
                    return

                print(f"âœ… ä¸»EXCELæ–‡ä»¶æ›´æ–°å®Œæˆ: æ–°å¢ {len(all_new_products)} æ¡æ•°æ®")

        except Exception as e:
            print(f"âŒ æ›´æ–°ä¸»EXCELæ–‡ä»¶å¤±è´¥: {e}")

    def _update_main_clicked(self, transfer_data: Dict):
        """æ›´æ–°ä¸»å·²ç‚¹å‡»å•†å“æ–‡ä»¶"""
        try:
            print(f"æ›´æ–°å·²ç‚¹å‡»å•†å“æ–‡ä»¶...")

            # æ”¶é›†æ‰€æœ‰æ–°ç‚¹å‡»çš„å•†å“ID
            all_new_clicked = []
            for browser_data in transfer_data['browsers'].values():
                all_new_clicked.extend(browser_data['new_clicked'])

            if not all_new_clicked:
                return

            # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å®‰å…¨
            with self._file_lock(self.main_clicked_file):
                # è¯»å–ç°æœ‰æ•°æ®
                if self.main_clicked_file.exists():
                    with open(self.main_clicked_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                else:
                    existing_data = {
                        'last_updated': datetime.now().isoformat(),
                        'hashes': [],
                        'total_count': 0
                    }

                # åˆå¹¶æ–°æ•°æ®
                existing_clicked = set(existing_data.get('hashes', []))
                new_clicked = set(all_new_clicked)
                combined_clicked = existing_clicked | new_clicked

                # æ›´æ–°æ•°æ®
                updated_data = {
                    'last_updated': datetime.now().isoformat(),
                    'hashes': sorted(list(combined_clicked)),
                    'total_count': len(combined_clicked),
                    'new_added': len(new_clicked)
                }

                # ä¿å­˜æ–‡ä»¶
                with open(self.main_clicked_file, 'w', encoding='utf-8') as f:
                    json.dump(updated_data, f, ensure_ascii=False, indent=2)

                print(f"âœ… å·²ç‚¹å‡»å•†å“æ–‡ä»¶æ›´æ–°å®Œæˆ: æ–°å¢ {len(new_clicked)} ä¸ªID")

        except Exception as e:
            print(f"âŒ æ›´æ–°å·²ç‚¹å‡»å•†å“æ–‡ä»¶å¤±è´¥: {e}")

    def _update_main_keywords(self, transfer_data: Dict):
        """æ›´æ–°ä¸»å·²æœç´¢å…³é”®è¯æ–‡ä»¶"""
        try:
            print(f"æ›´æ–°å·²æœç´¢å…³é”®è¯æ–‡ä»¶...")

            # æ”¶é›†æ‰€æœ‰æ–°æœç´¢çš„å…³é”®è¯
            all_new_keywords = []
            for browser_data in transfer_data['browsers'].values():
                all_new_keywords.extend(browser_data['new_keywords'])

            if not all_new_keywords:
                return

            # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å®‰å…¨
            with self._file_lock(self.main_keywords_file):
                # è¯»å–ç°æœ‰æ•°æ®
                if self.main_keywords_file.exists():
                    with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                else:
                    existing_data = {
                        'last_updated': datetime.now().isoformat(),
                        'searched_keywords': [],
                        'total_count': 0
                    }

                # åˆå¹¶æ–°æ•°æ®
                existing_keywords = set(existing_data.get('searched_keywords', []))
                new_keywords = set(all_new_keywords)
                combined_keywords = existing_keywords | new_keywords

                # æ›´æ–°æ•°æ®
                updated_data = {
                    'last_updated': datetime.now().isoformat(),
                    'searched_keywords': sorted(list(combined_keywords)),
                    'total_count': len(combined_keywords),
                    'new_added': len(new_keywords)
                }

                # ä¿å­˜æ–‡ä»¶
                with open(self.main_keywords_file, 'w', encoding='utf-8') as f:
                    json.dump(updated_data, f, ensure_ascii=False, indent=2)

                print(f"âœ… å·²æœç´¢å…³é”®è¯æ–‡ä»¶æ›´æ–°å®Œæˆ: æ–°å¢ {len(new_keywords)} ä¸ªå…³é”®è¯")
                
                # ğŸ”¥ æ›´æ–°å®Œæˆåç«‹å³åˆ·æ–°UI
                self._refresh_ui_searched_keywords()

        except Exception as e:
            print(f"âŒ æ›´æ–°å·²æœç´¢å…³é”®è¯æ–‡ä»¶å¤±è´¥: {e}")

    def _distribute_clicked_products(self):
        """åˆ†å‘å·²ç‚¹å‡»å•†å“åˆ°å„æµè§ˆå™¨ï¼ˆå›ä¼ è¦†ç›–æœºåˆ¶ï¼‰"""
        try:
            print(f"å¼€å§‹åˆ†å‘å·²ç‚¹å‡»å•†å“åˆ°å„æµè§ˆå™¨...")

            # è¯»å–ä¸»å·²ç‚¹å‡»å•†å“æ–‡ä»¶
            if not self.main_clicked_file.exists():
                print("â„¹ï¸ ä¸»å·²ç‚¹å‡»å•†å“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ†å‘")
                return

            with open(self.main_clicked_file, 'r', encoding='utf-8') as f:
                main_clicked_data = json.load(f)

            main_clicked_products = main_clicked_data.get('hashes', [])

            if not main_clicked_products:
                print("â„¹ï¸ æ²¡æœ‰å·²ç‚¹å‡»å•†å“éœ€è¦åˆ†å‘")
                return

            # åˆ†å‘åˆ°å„æµè§ˆå™¨ç›®å½•
            distributed_count = 0
            for browser_dir in self.scripts_dir.iterdir():
                if browser_dir.is_dir() and browser_dir.name.startswith('browser_'):
                    browser_id = browser_dir.name.replace('browser_', '')

                    # åˆ›å»ºæµè§ˆå™¨çš„å·²ç‚¹å‡»å•†å“æ–‡ä»¶
                    browser_clicked_file = browser_dir / "data" / "main_image_hashes.json"
                    browser_clicked_file.parent.mkdir(exist_ok=True)

                    # å‡†å¤‡æµè§ˆå™¨çš„å·²ç‚¹å‡»æ•°æ®
                    browser_clicked_data = {
                        'browser_id': browser_id,
                        'last_updated': datetime.now().isoformat(),
                        'hashes': main_clicked_products,
                        'total_count': len(main_clicked_products),
                        'source': 'main_distribution'
                    }

                    # è¦†ç›–ä¿å­˜åˆ°æµè§ˆå™¨ç›®å½•ï¼ˆåŠ æ–‡ä»¶é”é¿å…å¹¶å‘è¦†ç›–ï¼‰
                    with self._file_lock(browser_clicked_file):
                        with open(browser_clicked_file, 'w', encoding='utf-8') as f:
                            json.dump(browser_clicked_data, f, ensure_ascii=False, indent=2)

                    distributed_count += 1

            print(f"âœ… å·²ç‚¹å‡»å•†å“åˆ†å‘å®Œæˆ: åˆ†å‘åˆ° {distributed_count} ä¸ªæµè§ˆå™¨")

        except Exception as e:
            print(f"âŒ åˆ†å‘å·²ç‚¹å‡»å•†å“å¤±è´¥: {e}")

    def _file_lock(self, file_path: Path):
        """æ–‡ä»¶é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return FileLock(file_path)


class FileLock:
    """è·¨å¹³å°æ–‡ä»¶é”"""

    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.lock_file = file_path.with_suffix(file_path.suffix + '.lock')
        self.lock_handle = None

    def __enter__(self):
        try:
            self.lock_handle = open(self.lock_file, 'w')

            # ğŸ”¥ è·¨å¹³å°æ–‡ä»¶é”å¤„ç†
            if platform.system() == 'Windows' and HAS_MSVCRT:
                # Windowsæ–‡ä»¶é”
                msvcrt.locking(self.lock_handle.fileno(), msvcrt.LK_NBLCK, 1)
            elif HAS_FCNTL:
                # Linux/Macæ–‡ä»¶é”
                fcntl.flock(self.lock_handle.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            else:
                # æ— æ–‡ä»¶é”æ”¯æŒï¼Œä½¿ç”¨ç®€å•çš„æ–‡ä»¶å­˜åœ¨æ£€æŸ¥
                print("âš ï¸ æ–‡ä»¶é”ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•é”æœºåˆ¶")

            return self

        except (IOError, OSError) as e:
            if self.lock_handle:
                self.lock_handle.close()
            raise Exception(f"è·å–æ–‡ä»¶é”å¤±è´¥: {e}")

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.lock_handle:
            try:
                if platform.system() == 'Windows' and HAS_MSVCRT:
                    msvcrt.locking(self.lock_handle.fileno(), msvcrt.LK_UNLCK, 1)
                elif HAS_FCNTL:
                    fcntl.flock(self.lock_handle.fileno(), fcntl.LOCK_UN)
                # æ— æ–‡ä»¶é”æ—¶ä¸éœ€è¦ç‰¹æ®Šå¤„ç†
            except:
                pass
            finally:
                self.lock_handle.close()
                if self.lock_file.exists():
                    try:
                        self.lock_file.unlink()
                    except:
                        pass

    def _refresh_ui_searched_keywords(self):
        """ğŸ”¥ åˆ·æ–°UIä¸­çš„å·²æœç´¢å…³é”®è¯æ˜¾ç¤º"""
        try:
            if not self.ui_callback:
                print("â„¹ï¸ æœªè®¾ç½®UIå›è°ƒï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                return

            # è¯»å–ä¸»å·²æœç´¢å…³é”®è¯æ–‡ä»¶
            if not self.main_keywords_file.exists():
                print("â„¹ï¸ ä¸»å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                return

            with open(self.main_keywords_file, 'r', encoding='utf-8') as f:
                keywords_data = json.load(f)

            searched_keywords = set(keywords_data.get('searched_keywords', []))

            if not searched_keywords:
                print("â„¹ï¸ æ²¡æœ‰å·²æœç´¢å…³é”®è¯ï¼Œè·³è¿‡ç•Œé¢åˆ·æ–°")
                return

            # ğŸ”¥ è°ƒç”¨UIå›è°ƒå‡½æ•°åˆ·æ–°ç•Œé¢
            print(f"åˆ·æ–°UIæ˜¾ç¤º: {len(searched_keywords)} ä¸ªå·²æœç´¢å…³é”®è¯")
            self.ui_callback(searched_keywords)

        except Exception as e:
            print(f"âŒ åˆ·æ–°UIæ˜¾ç¤ºå¤±è´¥: {e}")

    def set_ui_callback(self, callback):
        """è®¾ç½®UIå›è°ƒå‡½æ•°"""
        self.ui_callback = callback
        print("âœ… UIå›è°ƒå‡½æ•°å·²è®¾ç½®")
